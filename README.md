# MIA Backend

A FastAPI backend for MIA (Decentralized AI Customer Support Assistant), where GPU miners run Mistral 7B Instruct locally to process multilingual customer support prompts.

## Features

- **Job Queue System**: Redis-based job queue for distributing chat requests to GPU miners
- **RESTful API**: FastAPI endpoints for chat, job management, and miner registration  
- **PostgreSQL Database**: Stores businesses, chat logs, and miner information
- **Docker Support**: Ready for Railway deployment
- **Health Checks**: Built-in health monitoring endpoints
- **GPU Miner Network**: Distributed network of GPU miners running Mistral 7B locally
- **Real-time Status**: Monitor miner status (idle/busy/offline)
- **Token Tracking**: Count tokens generated by each miner
- **Multilingual Support**: Process prompts in multiple languages

## API Endpoints

### Core Endpoints

- `POST /chat` - Submit a new chat message and create a job
- `GET /job/next` - Miners fetch the next available job
- `POST /job/result` - Miners submit job results
- `POST /register_miner` - Register a new GPU miner

### Miner Management Endpoints

- `POST /miner/{miner_id}/status` - Update miner status (idle/busy/offline)
- `GET /miners` - List all registered miners with their specs

### Utility Endpoints

- `GET /` - Basic health check
- `GET /health` - Detailed health check with service status
- `GET /metrics` - Get system metrics

## GPU Miner Architecture

The system uses distributed GPU miners running Mistral 7B locally:

### How It Works
1. **GPU miners** run Mistral 7B Instruct model locally using vLLM
2. **Registration**: Miners register with backend, reporting GPU specs
3. **Job Distribution**: Backend routes customer prompts to idle miners
4. **Local Inference**: Miners process prompts on GPU, no external APIs
5. **Response Return**: Generated text sent back with token count
6. **Status Tracking**: Real-time monitoring of miner availability

### Miner Requirements
- NVIDIA GPU with 8GB+ VRAM (RTX 3070, 3080, 3090, 4090, etc.)
- Ubuntu/Debian Linux with CUDA 11.7+
- 20-30GB storage (15GB minimum)
- Stable internet connection
- Python 3.8+

## üöÄ GPU Miner Installation

Install the MIA GPU miner with one command:

```bash
curl -sSL https://raw.githubusercontent.com/drapeaucharles/mia-backend/master/universal-installer.sh | bash
```

- ‚úÖ **Model:** Qwen2.5-7B-Instruct (Latest multilingual model)
- ‚úÖ **Speed:** 28-40 tokens/second on RTX 3090
- ‚úÖ **Backend:** Transformers with 4-bit quantization
- ‚úÖ **Requirements:** 8GB+ VRAM GPU
- ‚úÖ **Auto-registers** with MIA backend
- ‚úÖ **Starts earning** immediately

For detailed installation instructions, see [INSTALLATION.md](INSTALLATION.md)

## üìä Performance Comparison

| Setup | Speed | VRAM Required | Earnings Potential |
|-------|-------|---------------|-------------------|
| vLLM-AWQ | 60+ tok/s | 8GB+ | üí∞üí∞üí∞üí∞üí∞ Maximum |
| GGUF/llama.cpp | 20-40 tok/s | 4-6GB | üí∞üí∞üí∞ Good |
| Transformers 8-bit | 10-20 tok/s | 6GB+ | üí∞üí∞ Fair |
| Python 3.8 | 3-15 tok/s | 6GB+ | üí∞ Minimum |

## üéØ Special Setups

### Vast.ai with /data Volume
If your Vast.ai instance has a `/data` volume:
```bash
# The vLLM-AWQ installer automatically detects and uses /data
curl -s https://raw.githubusercontent.com/drapeaucharles/mia-backend/master/install-miner-vllm-awq-final.sh | bash
```
All files will be stored in `/data` to preserve disk space.

### Custom Installation Directory
```bash
# Download installer
wget https://raw.githubusercontent.com/drapeaucharles/mia-backend/master/install-miner-vllm-awq-final.sh
# Edit INSTALL_DIR in the script
nano install-miner-vllm-awq-final.sh
# Run
bash install-miner-vllm-awq-final.sh
```

## üèÉ After Installation - Managing Your Miner

### Starting the Miner

**For Vast.ai with /data:**
```bash
cd /data/mia-gpu-miner
./start_miner.sh
```

**For standard installations:**
```bash
cd ~/mia-gpu-miner
./start_miner.sh
```

### Monitoring Performance
```bash
# View real-time logs
tail -f /data/miner.log  # or ~/mia-gpu-miner/miner.log

# Check tokens/second
tail -f /data/miner.log | grep "tok/s"

# Check GPU usage
nvidia-smi -l 1
```

### Stopping the Miner
```bash
cd /data/mia-gpu-miner  # or ~/mia-gpu-miner
./stop_miner.sh
```

### Troubleshooting

**Slow speed (< 30 tok/s)?**
- You're probably not using vLLM-AWQ. Reinstall with Option 1.
- Check GPU: `nvidia-smi`
- Verify CUDA: `python3 -c "import torch; print(torch.cuda.is_available())"`

**Installation errors?**
- Python 3.8? Use Option 4
- Low VRAM? Use Option 2  
- Disk space issues? Ensure 20GB+ free

**Can't connect to backend?**
- Check internet: `ping google.com`
- Check logs: `tail -50 /data/miner.log`
- Firewall may block outgoing connections

## Project Structure

```
mia-backend/
‚îú‚îÄ‚îÄ main.py           # FastAPI application and routes
‚îú‚îÄ‚îÄ db.py            # SQLAlchemy models and database setup
‚îú‚îÄ‚îÄ redis_queue.py   # Redis queue implementation
‚îú‚îÄ‚îÄ schemas.py       # Pydantic models for request/response
‚îú‚îÄ‚îÄ utils.py         # Helper functions
‚îú‚îÄ‚îÄ Dockerfile       # Docker configuration for Railway
‚îú‚îÄ‚îÄ requirements.txt # Python dependencies
‚îú‚îÄ‚îÄ railway.json     # Railway deployment configuration
‚îú‚îÄ‚îÄ .env.example     # Environment variables template
‚îú‚îÄ‚îÄ .gitignore      # Git ignore patterns
‚îú‚îÄ‚îÄ install-miner.sh # One-line miner installer
‚îî‚îÄ‚îÄ README.md       # This file
```

## Setup

### Prerequisites

- Python 3.11+
- PostgreSQL
- Redis
- Docker (for containerized deployment)

### Local Development

1. Clone the repository:
```bash
git clone <repository-url>
cd mia-backend
```

2. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Set up environment variables:
```bash
cp .env.example .env
# Edit .env with your configuration
```

5. Start PostgreSQL and Redis:
```bash
# Using Docker
docker run -d -p 5432:5432 -e POSTGRES_PASSWORD=password postgres
docker run -d -p 6379:6379 redis
```

6. Run the application:
```bash
python main.py
# Or use uvicorn directly
uvicorn main:app --reload
```

The API will be available at `http://localhost:8000`

### API Documentation

Once running, access the interactive API documentation at:
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

## Docker Deployment

Build and run with Docker:

```bash
docker build -t mia-backend .
docker run -p 8000:8000 --env-file .env mia-backend
```

## Railway Deployment

1. Create a new Railway project
2. Add PostgreSQL and Redis services
3. Connect your GitHub repository
4. Railway will automatically detect the Dockerfile and deploy

Environment variables required in Railway:
- `DATABASE_URL` - PostgreSQL connection string (provided by Railway)
- `REDIS_URL` - Redis connection string (provided by Railway)

## Environment Variables

| Variable | Description | Example |
| `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@localhost/db` |
| `REDIS_URL` | Redis connection string | `redis://localhost:6379` |
| `PORT` | Server port (optional) | `8000` |

## Database Models

### Business
- `id`: Primary key
- `name`: Business name
- `contact_email`: Contact email
- `contact_phone`: Contact phone
- `created_at`: Creation timestamp
- `updated_at`: Last update timestamp

### ChatLog
- `id`: Primary key
- `session_id`: Chat session identifier
- `message`: Message content
- `role`: Message role (user/assistant)
- `timestamp`: Message timestamp
- `business_id`: Associated business (optional)

### Miner
- `id`: Primary key
- `name`: Miner name
- `auth_key`: Authentication key
- `ip_address`: Miner's IP address
- `gpu_name`: GPU model name
- `gpu_memory_mb`: GPU memory in MB
- `status`: Current status (idle/busy/offline)
- `job_count`: Number of jobs processed
- `total_tokens_generated`: Total tokens generated
- `created_at`: Registration timestamp
- `last_active`: Last activity timestamp

### SystemMetrics
- `id`: Primary key
- `metric_name`: Metric identifier
- `value`: Metric value
- `updated_at`: Last update timestamp

## Job Queue Structure

Jobs are stored in Redis with the following structure:
```json
{
  "job_id": "uuid",
  "prompt": "user message",
  "context": "optional context",
  "session_id": "uuid",
  "business_id": 1,
  "max_tokens": 500,
  "timestamp": "2024-01-01T00:00:00"
}
```

## Miner Communication Flow

1. **Registration**:
```json
POST /register_miner
{
  "name": "gpu-miner-001",
  "ip_address": "1.2.3.4",
  "gpu_name": "NVIDIA RTX 3090",
  "gpu_memory_mb": 24576,
  "status": "idle"
}
```

2. **Job Polling**:
```json
GET /job/next?miner_id=123
Response: {
  "job_id": "abc-123",
  "prompt": "Comment puis-je r√©initialiser mon mot de passe?",
  "context": "",
  "session_id": "xyz-789",
  "max_tokens": 500
}
```

3. **Result Submission**:
```json
POST /job/result
{
  "job_id": "abc-123",
  "session_id": "xyz-789",
  "output": "Pour r√©initialiser votre mot de passe...",
  "miner_id": "123",
  "success": true,
  "tokens_generated": 127
}
```

4. **Status Update**:
```json
POST /miner/123/status
{
  "status": "busy"
}
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## License

[Add your license here]